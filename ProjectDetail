(1) Q: Can you please explain your UART DCD project?
Sure. Junos support log of disconnect feature. Which means if system is configured with this feature and console is dropped (cable pull out for example)
all the sessions thorugh this console should be cleaned up. Typically, if there are sessions through the console terminal and the console is dropped off,
by default sessions through that console port are not cleaned up.
This causes a wastage of system resources as the sessions are not cleaned up and they are also useless as if console is dropped off, nothing can be done.

HW -- > DCD line --> CPLD --> GPIO--> irq --> driver--> UART core --> sigHup to application  

Generally speaking, what happens is that driver upon detecting DCD is lost on the line, inform the FreeBSD uart core layer. UART core then send the SIGHUP
(Hangup) to the respective application. Application can then do a session clean up which are established though the console terminal in question.
(A) Created interface by modifying GPIO driver code, which can configure the GPIO lines as interrupt lines.
(B) Added driver code to acheive the same.

BUG: First version of the HW was a faulty and was kind of generating interrupts continiously, causing instabilty and crash.
Fix: Probed the UART DCD lines  and verified that it generates spurious  interupts. Discussed the HW team and shown the traces
They fixed it in the HW.

(2) ISSU: How does ISSU works. Can you explain and a high level design flow explainig the details?
ISSU feature allows software upgrade between two Junos versions, without disuption is control path and minimal
disruption is data path.

What are requirements for ISSU: In the plateform there are few must pre-requisite for ISSU to take place.
a > Routing platform must have a redundant routing engine which plays a role of back-up during ISSU.

b > GRES : Graceful Routing Engine Switchover Concepts
======================================================

The graceful Routing Engine switchover (GRES) feature in Junos OS and Junos OS Evolved enables a device with redundant Routing Engines to continue forwarding 
packets even if one Routing Engine fails. GRES preserves interface and kernel information, and traffic is not interrupted. However, GRES does not preserve the
control plane.
Neighboring devices detect that the device has experienced a restart and react to the event in a manner prescribed by individual routing protocol specifications.
To preserve routing during a switchover, GRES must be combined with either:
++Graceful restart protocol extensions
++Nonstop active routing (NSR)

c > NSR :
=========
High availability feature that allows a routing platform with redundant Routing Engines to preserve routing information on the backup
Routing Engine and switch over from the primary Routing Engine to the backup Routing Engine without alerting peer nodes that a change 
has occurred. NSR uses the graceful Routing Engine switchover (GRES) infrastructure to preserve interface, kernel, and routing information.
Also known as nonstop routing (NSR).


Process : on cli, user have to execute "request system software in-service-upgrade" which triggeres a sequnce of events; 
=========

one : The primary Routing Engine validates the router configuration to ensure that it can be committed when you use the new software version.
====
=> Checks are made for the following:
=> Disk space is available for the /var file system on both Routing Engines.
=> The configuration is supported by a unified ISSU.
=> The PICs are supported by a unified ISSU.
=> Graceful Routing Engine switchover is enabled.
=> Nonstop active routing is enabled.

These checks are the same as the checks made when you enter the request system software validate in-service-upgrade command. If there is insufficient
disk space available on either of the Routing Engines, the unified ISSU process fails and returns an error message. However, unsupported PICs do not prevent
a unified ISSU. If there are unsupported PICs, the system issues a warning to indicate that these PICs will restart during the upgrade. Similarly, if there 
is an unsupported protocol configured, the system issues a warning that packet loss might occur for the unsupported protocol during the up¨.

two :  Figure 1: Device Status Before Starting a Unified ISSU
====
Device Status Before Starting a Unified ISSU


       |------------------|---------- <-------->|---fpc0---|
       |                  |  re0    | <-------->|---fpc1---|
       |                  |----------            ........... 
       |                  |----------            ...........
       |                  |   re1   |
       |------------------|----------


three: After the validation succeeds, the management process installs (copies) the new software image to the backup Routing Engine.
======

four: The backup Routing Engine is rebooted.
====

five : After the backup Routing Engine is rebooted and is running the new software, the kernel state synchronization process (ksyncd) synchronizes
=====
(copies) the configuration file and the kernel state from the primary Routing Engine.

six: After the configuration file and the kernel state are synchronized to the backup Routing Engine, the chassis process (chassisd) on the primary 
====
Routing Engine prepares other software processes for the unified ISSU. The chassis process informs the various software processes (such as rpd, apsd, bfdd, 
and so on) about the unified ISSU and waits for responses from them. When all the processes are ready, the chassis process sends an ISSU_PREPARE message to
the FPCs installed in the router. You can display the unified ISSU process messages by using the show log messages command.

seven: The Packet Forwarding Engine on each FPC saves its state and downloads the new software image from the backup Routing Engine. Next, each Packet Forwarding 
======
Engine sends an ISSU_READY message to the chassis process.

eight: After receiving an ISSU_READY message from a Packet Forwarding Engine, the chassis process sends an ISSU_REBOOT message to the FPC on which the Packet 
=====

Forwarding Engine resides. The FPC reboots with the new software image. After the FPC is rebooted, the Packet Forwarding Engine restores the FPC state, and a 
high-speed internal link is established with the backup Routing Engine running the new software. The chassis process link is also reestablished with the primary
Routing Engine.

nine: After all Packet Forwarding Engines have sent a READY message using the chassis process on the primary Routing Engine, other software processes are prepared 
=====
for a Routing Engine switchover. The system is ready for a switchover at this point.

ten: The Routing Engine switchover occurs, and the Routing Engine (re1) that was the backup now becomes the primary Routing Engine.
====

eleven: The new backup Routing Engine is now upgraded to the new software image. (This step is skipped if you have specified the no-old-master-upgrade option in the
=======

request system software in-service-upgrade command.)

twelve: 
=======

When the backup Routing Engine has been successfully upgraded, the unified ISSU is complete. 


OPTIMISATION  CODE FLOW : 
========================

According to ENS support for Dual load In-place Upgrade, below is the workflow for in-place upgrade with ENS

Disable ENS (In case of SmartNIC, disabling ENS is deferred to after step #5)
Register OLD NSX-T ENS module with RTM module. This will happen at module init time.
RTM will send PREDUALLOAD ioctl, upon this event, NSX-T ENS (FROM build) will unregister from ESX FPO service, and do other clean up, including closing char devices etc. FPO will be unregistered on old NSX-T ENS module. This means closing and disabling all FPO uplinks on every portset on the host. 
This means briefly, FPO will be completely disabled, as there is no NSX-T ENS module which is registered to its service.
Dual load ENS NSX-T module from TO build. New ENS module will register with RTM automatically on module init time. Also, automatically it will register with ESX FPO service at module init.
FPO will be enabled here and serviced by new NSX-T ENS module.
As ENS is disabled already, when other module needs to call into ENS, it uses VMKAPIs such as vmk_EnsSwitchCommand(), this call will fail. Traffic is flowing still in headless mode, and non ENS performance. (Slow path)
Upgrade portset  via RTM (Take portset write lock - block port – RTM save - hot swap – RTM restore – unblock- release portset write lock) Some outage here due to ports being blocked and hot swap.
Traffic is flowing still in headless mode, and non ENS performance.
Traffic will be flowing for that portset at ENS performance again. (Fast path)
Repeat steps 4-7 for each portset 
DFW port by port upgrade as per usual.
Unload old ENS NSX-T module once all above is done for all portsets 
nsx-opsagent starts and MP send hostConfig message to re-enable ENS on the portset, using the "TO build" ENS module. Some outage here as well due to driver switching. (In case of SmartNIC, ENS activation will be done after step #6)
From step #1 to #12, the dual load script will trigger:

At step #1, message exchanges among vmkernel, hostd, device manager and uplink helper thread to detach driver from pNIC device, and re-attach with ENS mode disabled.
Dual load script sends ENS deactivate/activate command to vmkernel portset layer.
Portset layer notifies host to refresh pNIC driver on the uplink.
Hostd sends an unbind and a rescan request to device manager.
In response to the unbind request, device manager quiesce and detach driver from the device.
In response to the rescan request, device manager calls scan device on device's parent, to trigger the re-creation of device object and the initialization of device in non-ENS mode.
Driver creates uplink logical device that will be claimed by uplink driver, which follows native driver model to init the device in uplink helper context.
Uplink will be in io-able state in non-ENS mode. 
Uplink continues forwarding traffic in non-ENS mode from step #2 to #7.
At step #12, similar operations as above will perform, except that pNIC will switch back to ENS mode.
During uplink's switching between ENS and non-ENS mode, packets sent to uplink and received from uplink will be dropped. To avoid this switching, we can:



Change the order of operations in existing in-place upgrade workflow, to reduce the duration of ENS deactivation, to avoid extra works in ESX platform that process ENS uplink traffic while ENS is deactivated.

Detach uplink from ENS first prior to step #1, but still keep it in ENS mode. As a result, FPO will be cleaned up, all Tx and Rx queues will be quiesced and released.
FPO cleanup will be called in the "FROM build" ENS module.
Quiesce and destroy will be called for each Tx and Rx queue from QM in the "FROM build" ENS module.
Deconfigure queue will be called for the device in the "FROM build" ENS module.
Place uplink is a new mode, call "ENS ready" mode.
When ENS is disabled at step #1
Portset layer identifies uplinks are in "ENS ready" mode, instead of notifying hostd to refresh pNIC driver, it will simply skip the uplink.
Similarly, when hostd receives a portset level ENS deactivate message, it should skip uplinks in "ENS ready" mode.
At step #6, after FPO service is unregistered in "FROM build", and "TO build" ENS is dual loaded and hot swapped to "TO build" vswitch. Activate ENS using "TO build" ENS module in a new headless mode, where it only allows uplink to attach and skip software flow cache. This is to avoid extra work at ESX platform to create a separate context to poll ENS uplink in proposal #1.
Attach uplink to ENS
Portset layer checks uplink's state, if it's in "ENS ready" mode, don't notify hostd to refresh pNIC driver. Instead, attach uplink to ENS directly.
ENS QM and FPO initialize the uplink as usual.
As flow cache is disabled, packets poll from uplink will be punted to slow path directly in uplink's Rx networld or poll world context.
Similarly for Tx path, as non uplink ports are not attached to ENS, their Tx world polls Tx packets from the port client, injects into virtual switch and hands over to ENS at ENS's uplink output IOChain. Tx packets will be queued in uplink's temp list and picked up by uplink's Tx networld or poll world and sent to pNIC driver.
So, slow path will be handled by "TO build" NSX modules.
As ENS already activated, at #12 of in-place upgrade process, activating ENS is not required. Instead, ENS can leave the new mode and start to accept non-uplink port's attach request. So, upgrade script will iterate ports on the portset and attach all non-uplink ports.

Maintain traffic between PktHandle stack and ENS driver. (See PR 3380485) 
As uplink remains in ENS mode but the stack is in PktHandle mode, no uplink traffic is allowed due to lack of packet format conversion and proper context to handle them.
To maintain minimum traffic between stack and driver, we need:
The default RxQ/RSS engine are still valid during in-place upgrade.
The "from build" ENS needs to stop the default queue(s) and backup the state (queue index and queue ID) on the platform while stopping and releasing other Rx queues.
The "to build" ENS skips setup queue call for these default queues and only call start queue on them.
The minimum number of Tx queues (which is required when HClk MQ is enabled) are still valid during in-place upgrade.
The "from build" ENS needs to stop them and backup their state (queue index and queue ID) on the platform while stopping and releasing other Tx queues.
The "to build" ENS skips setup queue call for these default queues and only call start queue on them.
Platform needs to backup and restore state and maintain traffic
Backup data through ENS passthrough command on the "from build" ENS before deconfigure queue command, and restore it on the "to build" ENS before the configure queue command.
Datapath will use VM's Tx world for Tx, and driver's NetPoll world for Rx
For Tx path:
After uplink detaches from ENS, installs an IO chain to intercept all Tx packets. PktHandle to Mbuf conversion happens in the callback of this IOChain.
Restarts all Tx queues.
In case of HClk is disabled, add a per Tx queue lock to serialize all Tx calls into driver.
In HClk is enabled, synchronization is done in HClk.
For Tx completion, if HClk is enable, driver activates NetPoll, which will call HClk's ENS completion callback.
For Tx completion, if HClk is disabled, NetPoll world will be waken up and holds the per Tx queue lock, calls into driver to complete Tx packets.
For Rx path:
Wake up NetPoll world instead of ENS networld
Replace the NetPoll poll driver callback, instead of calling PktHandle driver poll callback, it calls ENS driver's burst Rx. And converts Rx mbuf to PktHandle and queue them in NetPoll's Rx packet list. So it can reuse NetPoll's existing code to push the packets into the stack.

Benefits
Virtual LAG
Customers use 2/4 x 10, 2/4 x 25, 2/4 x 50, 2/4 x 100 network cards in their environment for bandwidth/availability reasons. The expectation is that workloads can use close to the available bandwidth when there is no contention and reasonable bandwidth when there is. M-LAG/MC-LAG/DT/VLT etc.. from different underlay vendors needs to work with the LACP implementation of ESX to make this work. This often makes things complex needing interlock with underlay network administrators during provisioning and day 2 operations, as such is discouraged by NSX reference design.
The solution which NSX currently has is to configure the uplinks as active-active, which means we end up creating as many TEPs as the uplink. The workload VMs are placed statically behind a single TEP for its lifetime on the Host, which means the maximum TX/RX throughput available to a workload is limited by the individual ethernet PHY speed. This is not ideal. Since the VM to VTEP mapping is static, the throughput goes down by an additional factor of N for all workloads if N VMs, which got mapped to the same TEP, start TX/RX. So, on a system with 100G available bandwidth, a workload VM can end up seeing only 5 or 10Gbps throughput, this is unacceptable.
TEP Groups allows overlay load-balancing at the granularity of flow, utilizing available link capacity. Designed to work with any scale up solutions like UENS/SmartNIC. Can support any TN form-factor(ESX, Edge VM, BM Edge). Allows for maximum link utilization for single VM as well as during contention with other VMs. Replace M-LAG requirement/complexity/inter-lock with underlay vendors/admins for NSX provisioning, true decoupling of overlay/underlay.  


Bridge --- 

driver1 --> psuedo bridge  = driver 2
driver2 --> pseudo bridge =  driver 1



cleanup path : 

get w lock

........ destroy engines...

........ other clean up
rel w lock
destroy lock
init_done = FALSE



dispatch path :

if ! init_done
     return;



After clean up:

get w lock
fo->init_done = false
...... destry engine
....... other clean up

rel w lock
destroy lock


dispatch path:

get r lock

if ! fo_init_done
    rel lock
    return;





